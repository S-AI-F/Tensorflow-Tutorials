{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we attempt tp classify movie reviews as *positive* or *negative* based on the text description in the review. So we will work on a two-class classification problem.\n",
    "\n",
    "This notebook presents a besic application of transfer learning using TensorFlow Hub and Keras.\n",
    "\n",
    "The data used here is the IMDB dataset. It contains 50,000 movie reviews. They are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets contain an equivalent number of positive and negative reviews: They are balancd.\n",
    "\n",
    "We will use:\n",
    "\n",
    "* `tf.keras` to build and train models in TensorFlow\n",
    "* `TensorFlow Hub` a library and platform for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.1.0\n",
      "Eager mode:  True\n",
      "Hub version:  0.8.0\n",
      "GPU is NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "!pip install -q tensorflow-hub\n",
    "!pip install -q tfds-nightly\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the IMDB dataset\n",
    "The IMDB dataset is available on TensorFlow datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset imdb_reviews/plain_text/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to C:\\Users\\saif\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec90288ce07c47698dcdafd2ca6cf1fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84d5fbe2c794d22a4c6214eadb585f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Shuffling and writing examples to C:\\Users\\saif\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incomplete2W875L\\imdb_reviews-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d31c18b98d498e89ced61738f381ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Shuffling and writing examples to C:\\Users\\saif\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incomplete2W875L\\imdb_reviews-test.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8eb9f45f75487a98d071f60f106d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Shuffling and writing examples to C:\\Users\\saif\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incomplete2W875L\\imdb_reviews-unsupervised.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685d20a6ff9348f5858bb039aa8ff5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to C:\\Users\\saif\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Split the training set into 60% and 40%, so e'll end up with 15,000 examples \n",
    "# for training, 10,000 examples for validation and 25,000 examples for testing\n",
    "\n",
    "train_data, validation_data, test_data = tfds.load(\n",
    "    name=\"imdb_reviews\", \n",
    "    split=('train[:60%]', 'train[60%:]', 'test'),\n",
    "    as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the data\n",
    "Each example is a sentence representing the movie review and a corresponding label. \n",
    "The label is an an integer of either 0 or 1, where 0 is a negative review, and 1 is a positive review.\n",
    "Le's print the first 5 examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=string, numpy=\n",
       "array([b'This is a big step down after the surprisingly enjoyable original. This sequel isn\\'t nearly as fun as part one, and it instead spends too much time on plot development. Tim Thomerson is still the best thing about this series, but his wisecracking is toned down in this entry. The performances are all adequate, but this time the script lets us down. The action is merely routine and the plot is only mildly interesting, so I need lots of silly laughs in order to stay entertained during a \"Trancers\" movie. Unfortunately, the laughs are few and far between, and so, this film is watchable at best.',\n",
       "       b\"Perhaps because I was so young, innocent and BRAINWASHED when I saw it, this movie was the cause of many sleepless nights for me. I haven't seen it since I was in seventh grade at a Presbyterian school, so I am not sure what effect it would have on me now. However, I will say that it left an impression on me... and most of my friends. It did serve its purpose, at least until we were old enough and knowledgeable enough to analyze and create our own opinions. I was particularly terrified of what the newly-converted post-rapture Christians had to endure when not receiving the mark of the beast. I don't want to spoil the movie for those who haven't seen it so I will not mention details of the scenes, but I can still picture them in my head... and it's been 19 years.\",\n",
       "       b'Hood of the Living Dead had a lot to live up to even before the opening credits began. First, any play on \"...of the living dead\" invokes His Holiness Mr. Romero and instantly sets up a high standard to which many movies cannot afford to aspire. And second, my movie-watching companion professed doubt that any urban horror film would surpass the seminal Leprechaun In the Hood. Skeptical, we settled in to watch. <br /><br />We were rewarded with a surprisingly sincere and good-hearted zombie film. Oh, certainly the budget is low, and of course the directors\\' amateurs friends populate the cast, but Hood of the Living Dead loves zombie cinema. Cheap? Yeah. But when it\\'s this cheap, you can clearly see where LOVE holds it together. <br /><br />Ricky works in a lab during the day and as a surrogate parent to his younger brother at night. He dreams of moving out of Oakland. Before this planned escape, however, his brother is shot to death in a drive-by. Ricky\\'s keen scientific mind presents an option superior to CPR or 911: injections of his lab\\'s experimental regenerative formula. Sadly, little bro wakes up in an ambulance as a bloodthirsty Oakland zombie! Chaos and mayhem! I think it\\'s more economical to eat your enemies than take vengeance in a drive-by, but then again, I\\'m a poor judge of the complexities of urban life. (How poor a judge? In response to a gory scene involving four men, I opined \"Ah-ha! White t-shirts on everyone so the blood shows up. Economical! I used the same technique in my own low-budget horror film.\" Jordan replied, \"No, that\\'s gang dress. White t-shirts were banned from New Orleans bars for a time as a result.\" Oh.)<br /><br />A lot of the movie is set in someone\\'s living room, so there\\'s a great deal of hanging out and waiting for the zombies. But the characters are sympathetic and the movie is sincere-- it surpasses its budget in spirit. <br /><br />Zombie explanation: When man plays God, zombies arise! Or, perhaps: Follow FDA-approved testing rules before human experimentation! <br /><br />Contribution to the zombie canon: This is the first zombie movie I\\'ve seen with a drive-by shooting. As far as the actual zombies go, infection is spread with a bite as usual, but quite unusually head shots don\\'t work-- it\\'s heart shots that kill. Zombies have pulses, the absence of which proves true death. And these zombies make pretty cool jaguar-growl noises. <br /><br />Gratuitous zombie movie in-joke: A mercenary named Romero. Groan. <br /><br />Favorite zombie: Jaguar-noise little brother zombie, of course!',\n",
       "       b\"For me this is a story that starts with some funny jokes regarding Franks fanatasies when he is travelling with a staircase and when he is sitting in business meetings... The problem is that when you have been watching this movie for an hour you will see the same fantasies/funny situations again and again and again. It is to predictable. It is more done as a TV story where you can go away and come back without missing anything.<br /><br />I like Felix Herngren as Frank but that is not enough even when it is a comedy it has to have more variations and some kind of message to it's audience....<br /><br />\",\n",
       "       b'This is not a bad movie. It follows the new conventions of modern horror, that is the movie within a movie, the well known actress running for her life in the first scene. This movie takes the old convention of a psycho killer on he loose, and manage to do something new, and interesting with it. It is also always nice to see Molly Ringwald back for the attack.<br /><br />So this might be an example of what the genre has become. Cut hits all the marks, and is actually scary in some parts. I liked it I gave it an eight.'],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples_batch, train_labels_batch = next(iter(train_data.batch(5)))\n",
    "train_examples_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the first 10 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([0, 0, 1, 0, 1], dtype=int64)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model\n",
    "The neural network is creaed by stcking layers. This requires 3 main architectural decisions:\n",
    "* How to represent the text?\n",
    "* How many layers we use in the model?\n",
    "* How many *hidden units* we use for each layer?\n",
    "\n",
    "In our example, the input data consists of sentences. The labels to predict are either 0 or 1.\n",
    "\n",
    "One way to represent the text is to convert sentences into embeddings vectors. We can use a pre-trained text embedding as the first layer, which will have 3 advantages:\n",
    "* we don't have to worry about text preprocessing\n",
    "* we can benefit from transfer learning\n",
    "* the embedding has a fixed size, so it is simpler to process\n",
    "\n",
    "For this example we will use a **pretrained text embedding model** from TensorFlow Hub called **google/tf2-preview/gnews-swivel-20dim/1**\n",
    "\n",
    "There are 3 other pre-trained models to test:\n",
    "* **google/tf2-preview/gnews-swivel-20dim-with-oov/1** - same as **google/tf2-preview/gnews-swivel-20dim/1**, but with 2.5% vocabulary converted to OOV buckets. This can help if vocabulary of the task and vocabulary of the model don't fully overlap.\n",
    "* **google/tf2-preview/nnlm-en-dim50/1** - A much larger model with ~1M vocabulary size and 50 dimensions.\n",
    "* **google/tf2-preview/nnlm-en-dim128/** - Even larger model with ~1M vocabulary size and 128 dimensions.\n",
    "\n",
    "Let's first create a Keras layer that uses a TensorFlow Hub model to embed the sentences, and try it out on a couple of input examples. Note that no matter the length of the input text, the output shape of the embeddings is: `(num_examples, embedding_dimension)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 20), dtype=float32, numpy=\n",
       "array([[ 2.209591  , -2.7093675 ,  3.6802928 , -1.0291991 , -4.1671185 ,\n",
       "        -2.4566064 , -2.2519937 , -0.36589956,  1.9485804 , -3.1104462 ,\n",
       "        -2.4610963 ,  1.3139242 , -0.9161584 , -0.16625322, -3.723651  ,\n",
       "         1.8498232 ,  3.499562  , -1.2373022 , -2.8403084 , -1.213074  ],\n",
       "       [ 1.9055302 , -4.11395   ,  3.6038654 ,  0.28555924, -4.658998  ,\n",
       "        -5.5433393 , -3.2735848 ,  1.9235417 ,  3.8461034 ,  1.5882455 ,\n",
       "        -2.64167   ,  0.76057523, -0.14820506,  0.9115291 , -6.45758   ,\n",
       "         2.3990374 ,  5.0985413 , -3.2776263 , -3.2652326 , -1.2345369 ],\n",
       "       [ 3.6510668 , -4.7066135 ,  4.71003   , -1.7002777 , -3.7708545 ,\n",
       "        -3.709126  , -4.222776  ,  1.946586  ,  6.1182513 , -2.7392752 ,\n",
       "        -5.4384456 ,  2.7078724 , -2.1263676 , -0.7084146 , -5.893995  ,\n",
       "         3.1602864 ,  3.8389287 , -3.318196  , -5.1542974 , -2.4051712 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "                           dtype=tf.string, trainable=True)\n",
    "hub_layer(train_examples_batch[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's now build the full mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     (None, 20)                400020    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                336       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 400,373\n",
      "Trainable params: 400,373\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layers are stacked sequenially to build the classifier:\n",
    "1. The first layer is a Tensorflow Hub layer. This layer uses a pre-trained Saved Model to map^a sentence into its embedding vector. The pre-trained text embedding model that we are using (google/tf2-preview/gnews-swivel-20dim/1) splits the sentence into tokens, embeds each token and then combines the embedding. The resulting dimensions are: (num_examples, embedding_dimension)\n",
    "2. The fixed-length output vector is piped througha fully-connected (Dense) layer with16 hidden units\n",
    "3. The last layer is densely connected with a single output node\n",
    "\n",
    "Let's compile the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function and optimizer\n",
    "A model needs a loss function and an optimizer for training. Since we are performing a binary classification and the model ouputs are probabilities (a single-unit layer with a sigmoid activation), we will use the `binary_crossentropy` loss function.\n",
    "\n",
    "We can use other loss function, such as `mean_squared_error`. But, `binary_crossentropy` performs better when dealing with probabilities (it measures the distance between probability distributions). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Train the model for 20 epochs in mini-batches of 512 samples. This is 20 iterations over all samples in the `x_train` and `y_train` tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 5s 181ms/step - loss: 0.9857 - accuracy: 0.5022 - val_loss: 0.7444 - val_accuracy: 0.5191\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 4s 124ms/step - loss: 0.6764 - accuracy: 0.5841 - val_loss: 0.6530 - val_accuracy: 0.5925\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 4s 123ms/step - loss: 0.6284 - accuracy: 0.6220 - val_loss: 0.6219 - val_accuracy: 0.6239\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 4s 123ms/step - loss: 0.5971 - accuracy: 0.6541 - val_loss: 0.5925 - val_accuracy: 0.6525\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 4s 123ms/step - loss: 0.5658 - accuracy: 0.6802 - val_loss: 0.5621 - val_accuracy: 0.6759\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 4s 122ms/step - loss: 0.5327 - accuracy: 0.7101 - val_loss: 0.5296 - val_accuracy: 0.7064\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 4s 126ms/step - loss: 0.4912 - accuracy: 0.7483 - val_loss: 0.4887 - val_accuracy: 0.7588\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 4s 122ms/step - loss: 0.4470 - accuracy: 0.7869 - val_loss: 0.4537 - val_accuracy: 0.7769\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 4s 120ms/step - loss: 0.4083 - accuracy: 0.8119 - val_loss: 0.4238 - val_accuracy: 0.7992\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 4s 120ms/step - loss: 0.3737 - accuracy: 0.8327 - val_loss: 0.3994 - val_accuracy: 0.8181\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 4s 121ms/step - loss: 0.3446 - accuracy: 0.8501 - val_loss: 0.3779 - val_accuracy: 0.8286\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 4s 124ms/step - loss: 0.3165 - accuracy: 0.8665 - val_loss: 0.3595 - val_accuracy: 0.8311\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 4s 122ms/step - loss: 0.2922 - accuracy: 0.8785 - val_loss: 0.3453 - val_accuracy: 0.8395\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 4s 122ms/step - loss: 0.2714 - accuracy: 0.8885 - val_loss: 0.3342 - val_accuracy: 0.8439\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 4s 121ms/step - loss: 0.2530 - accuracy: 0.8993 - val_loss: 0.3254 - val_accuracy: 0.8476\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 4s 123ms/step - loss: 0.2351 - accuracy: 0.9058 - val_loss: 0.3174 - val_accuracy: 0.8595\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 4s 120ms/step - loss: 0.2205 - accuracy: 0.9143 - val_loss: 0.3116 - val_accuracy: 0.8609\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 4s 120ms/step - loss: 0.2064 - accuracy: 0.9213 - val_loss: 0.3074 - val_accuracy: 0.8613\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 4s 121ms/step - loss: 0.1939 - accuracy: 0.9277 - val_loss: 0.3037 - val_accuracy: 0.8655\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 4s 121ms/step - loss: 0.1825 - accuracy: 0.9335 - val_loss: 0.3039 - val_accuracy: 0.8709\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data.shuffle(10000).batch(512),\n",
    "                   epochs = 20,\n",
    "                   validation_data=validation_data.batch(512),\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "Now we will assess the model performance based on two metrics: Loss (the error), and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.319\n",
      "accuracy: 0.862\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data.batch(512), verbose=2)\n",
    "\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "    print(\"%s: %.3f\" % (name, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
